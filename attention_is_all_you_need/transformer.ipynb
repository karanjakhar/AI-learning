{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_df = pd.read_csv('data/companies_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies_df['name'][:1000].to_csv('companies_names_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_names = companies_df['name'][:1000].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' '.join(company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(''.join(company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chars = sorted(list(set(''.join(company_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(''.join(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoi = {s:i for i,s in enumerate(all_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos = {i:s for i,s in enumerate(all_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in company_names:\n",
    "#     print(name)\n",
    "#     print(encode(name))\n",
    "#     print(decode(encode(name)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nn.Embedding(vocab_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = token_embedding(torch.tensor([23,16,27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = torch.stack((torch.tensor([23,16,27]), torch.tensor([23,16,27])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model=512, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        out = (x - mean)/torch.sqrt(var + self.eps)\n",
    "        out = out * self.gamma + self.beta\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model,d_ff )\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int = 8, d_model: int = 512):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.heads = heads\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(16,16)).view(1,1,16,16))\n",
    "\n",
    "    def forward(self, k: torch.tensor, v: torch.tensor, q: torch.tensor, mask: bool =  False):\n",
    "\n",
    "        k = self.split(self.k(k))\n",
    "        v = self.split(self.v(v))\n",
    "        q = self.split(self.q(q))\n",
    "\n",
    "\n",
    "\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        # print(q.shape, k_t.shape)\n",
    "        scale = (q@k_t)*(k.shape[-1]**-0.5)\n",
    "        # print(scale.shape)\n",
    "        if mask:\n",
    "            B, nH, T, Hs = scale.shape \n",
    "            scale = scale.masked_fill(self.mask[:,:,:T,:T] == 0, -10000)\n",
    "            # print('scale shape:', scale.shape)\n",
    "            # print('scale values:', scale)\n",
    "\n",
    "        score = self.softmax(scale)\n",
    "\n",
    "        scale_product = score @ v\n",
    "        \n",
    "        batch,heads, length, d_tensor = scale_product.size()\n",
    "        scale_product = scale_product.transpose(1, 2).contiguous().view(batch, length, heads*d_tensor)\n",
    "        # print(scale_product.shape)\n",
    "        scale_product = self.w_concat(scale_product)\n",
    "\n",
    "        return scale_product\n",
    "    \n",
    "    def split(self, tensor:torch.tensor):\n",
    "\n",
    "        batch, length, d_model = tensor.size()\n",
    "        d_tensor = d_model//self.heads\n",
    "\n",
    "        new_tensor = tensor.view(batch, length, self.heads, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return new_tensor\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "vocab_size = 51\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n_layers=2, drop_prob=0.1, vocab_size=51, d_model=512, device='cpu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.ffn = PositionwiseFeedForward()\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            _x = x\n",
    "            x = self.attention(x,x,x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_norm(x + _x)\n",
    "\n",
    "            _x = x\n",
    "            x = self.ffn(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_norm(x + _x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder_model(s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2478,  1.3195, -1.1371,  ...,  0.3854, -0.9986,  1.0983],\n",
       "         [ 1.4962, -0.1559,  0.5545,  ...,  0.7497, -0.6175,  0.3648],\n",
       "         [-0.7865,  0.1280,  1.4923,  ..., -0.6512,  0.6116, -1.5909]],\n",
       "\n",
       "        [[-0.1754,  1.5139, -1.0582,  ...,  0.8110, -1.3169,  0.9074],\n",
       "         [ 1.5260,  0.0705, -0.0097,  ...,  0.5855, -0.5872,  0.5131],\n",
       "         [-0.6733,  0.2820,  1.5630,  ..., -0.7395,  0.1737, -1.4332]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (token_embedding): Embedding(51, 512)\n",
      "  (positional_embedding): Embedding(51, 512)\n",
      "  (attention): MultiHeadAttention(\n",
      "    (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (ffn): PositionwiseFeedForward(\n",
      "    (l1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (l2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layer_norm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in encoder_model.parameters()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(\n",
    "\tp.numel() for p in encoder_model.parameters() if p.requires_grad\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 3202048, Trainable Parameters: 3202048\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Parameters: {total_params}, Trainable Parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(encoder_model, s_t, 'encoder.onnx', input_names=[\"in_tokens\"], output_names=[\"out_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_layers=2, vocab_size=51, d_model=512,drop_prob=0.1, device='cpu') -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.positional_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.ffn = PositionwiseFeedForward()\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x,k,v):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x) \n",
    "        pos_emb = self.positional_embedding(torch.arange(T, device=self.device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            _x = x\n",
    "            x = self.attention(x,x,x, True)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_norm(x + _x)\n",
    "\n",
    "            _x = x\n",
    "            x = self.attention(k,v,x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_norm(x + _x)\n",
    "\n",
    "            _x = x\n",
    "            x = self.ffn(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_norm(x + _x)\n",
    "\n",
    "            \n",
    "        return x\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23, 16, 27],\n",
       "        [23, 16, 27]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_output = decoder(s_t, output, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, d_model=512, vocab_size=51) -> None:\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        output = self.decoder(x, encoder_output, encoder_output)\n",
    "        output = self.lm_head(output)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder_model = EncoderDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder_decoder_model(s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 51])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 51])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.contiguous().view(-1, output.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = torch.stack((torch.tensor([23,16,27]), torch.tensor([23,16,27])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "positional_embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = token_embedding(s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7757,  0.0586,  0.2674,  ..., -0.5857, -0.2443, -0.2355],\n",
       "         [ 0.6047, -0.5344,  0.9430,  ...,  0.1872, -0.5387,  1.2751],\n",
       "         [-0.0704, -0.5737,  0.0403,  ...,  0.9521,  0.6735,  0.7023]],\n",
       "\n",
       "        [[-1.7757,  0.0586,  0.2674,  ..., -0.5857, -0.2443, -0.2355],\n",
       "         [ 0.6047, -0.5344,  0.9430,  ...,  0.1872, -0.5387,  1.2751],\n",
       "         [-0.0704, -0.5737,  0.0403,  ...,  0.9521,  0.6735,  0.7023]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = positional_embedding(torch.arange(s_t.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2713, -1.6347, -1.1250,  ..., -1.2926,  0.1686,  0.3813],\n",
       "        [ 0.7880, -0.4153, -0.7522,  ..., -0.9934,  1.1213,  0.2822],\n",
       "        [ 0.6961, -0.2195,  0.0581,  ..., -1.2911,  1.7523, -0.0187]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [4, 6]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tok_emb + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = nn.Linear(d_model, d_model)(input_1)\n",
    "v = nn.Linear(d_model, d_model)(input_1)\n",
    "q = nn.Linear(d_model, d_model)(input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_t = torch.rand([2,3,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0802, 0.0982, 0.4308,  ..., 0.7891, 0.7740, 0.0154],\n",
       "         [0.6104, 0.0826, 0.5575,  ..., 0.5876, 0.5308, 0.4425],\n",
       "         [0.9638, 0.9295, 0.8518,  ..., 0.3090, 0.9917, 0.1731]],\n",
       "\n",
       "        [[0.6728, 0.1163, 0.8021,  ..., 0.6018, 0.2805, 0.0305],\n",
       "         [0.7369, 0.1752, 0.8285,  ..., 0.2431, 0.1567, 0.5318],\n",
       "         [0.6567, 0.3164, 0.1286,  ..., 0.1141, 0.1067, 0.6810]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_t_m = temp_t.view(2,3,heads, 512//heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_t_m.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k.view(2,3,heads, 512//heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.view(2,3,heads, 512//heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = v.view(2,3,heads, 512//heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_t = k.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k_t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.2117,  -0.3581,  -0.4260,  -0.2827,  -2.8279,  -2.1437,  -2.8679,\n",
       "            -1.0343],\n",
       "          [  7.1344,   3.5787,   0.3892,   3.0862,  -5.9506,  -2.9849,   2.5459,\n",
       "             3.6199],\n",
       "          [  5.2800,   1.7048,  -4.2753,   3.9341,  -1.3720,   5.1133,  -4.5258,\n",
       "             6.0240],\n",
       "          [  0.0952,  -0.1833,   7.6764,  -1.8878,   4.6266,  -0.9071,   0.0241,\n",
       "            -1.9372],\n",
       "          [  3.7228,   7.9129,  -6.5832,   6.3541,  -3.6942,   4.2208,   2.3070,\n",
       "            -1.6370],\n",
       "          [ -5.9752,   1.5262,  -2.4834,   1.4054,  -7.2886,   0.4416,   0.1225,\n",
       "            -1.8685],\n",
       "          [ -2.5396,   3.4102,  -1.2021,   1.1720,   0.7296,   0.8213,  10.1474,\n",
       "             1.8474],\n",
       "          [ -0.6137,  -4.1179,   0.0646,   4.4747,  -0.4499,   9.5888,  -2.5183,\n",
       "            -5.3273]],\n",
       "\n",
       "         [[ -1.2489,  -1.0893,   8.4847,  -1.0904,   3.6712,   6.9259,  -3.6202,\n",
       "             8.8685],\n",
       "          [ -7.5599,  -5.0985,  -5.1248,   2.2981,   0.1744,  -6.4129,   2.2523,\n",
       "            -3.8481],\n",
       "          [  4.6210,  -5.8682,   4.1971,   1.6684,   2.7223,   3.3546,  -0.5590,\n",
       "             7.0851],\n",
       "          [  4.7820,   5.9406,   3.9955,   3.1988,   3.4751,   1.8887,  -9.2231,\n",
       "            -2.7367],\n",
       "          [ -0.5250,   5.7303,   5.5978,   9.2771,   2.8471,   0.4151,  -8.0212,\n",
       "             1.2914],\n",
       "          [  1.7495,   1.4027,   1.6970,   0.0891,   1.2074,   4.4429,   1.1441,\n",
       "            -2.6519],\n",
       "          [ -5.9957,   1.6356,   1.2607,   5.9190,  -7.5994,   4.5038,   7.3015,\n",
       "            -0.3216],\n",
       "          [ -1.8147,   1.0672, -10.6744,   0.6305,  -1.6516,   4.5668,  -3.7244,\n",
       "             2.4485]],\n",
       "\n",
       "         [[  2.5276,   8.1820,  -7.1408,  -0.7410,  -7.3672,   0.5742,  -2.3389,\n",
       "            -5.7475],\n",
       "          [ -5.0144,  -0.4770,   4.1695, -11.5607,  -4.7276,   4.3123,   3.5008,\n",
       "            -1.8475],\n",
       "          [  1.4967,   0.9426,  -3.1709,   4.4125,   0.4389,  -3.3159,  -5.3993,\n",
       "            -4.3383],\n",
       "          [  4.3703,   3.3220,   8.6575,   4.0856,   7.0397,   3.6803,  -8.0119,\n",
       "            -1.7631],\n",
       "          [  5.8835,   2.2541,   2.5373, -12.4686,  10.5646,  -1.9911,   2.8541,\n",
       "             3.0369],\n",
       "          [  9.3080,   0.7189,  -0.4061,   5.3547,   4.2838, -12.6600,  -1.9116,\n",
       "           -10.2533],\n",
       "          [  1.3250,  11.1266,  -4.8665,   4.0605,   0.2316,  -6.4047,   5.0330,\n",
       "            -5.2616],\n",
       "          [ -4.0703,  -4.0169,  -0.9504,  -1.9907,  -2.4265,   0.6058, -10.1534,\n",
       "             0.3430]]],\n",
       "\n",
       "\n",
       "        [[[  1.2117,  -0.3581,  -0.4260,  -0.2827,  -2.8279,  -2.1437,  -2.8679,\n",
       "            -1.0343],\n",
       "          [  7.1344,   3.5787,   0.3892,   3.0862,  -5.9506,  -2.9849,   2.5459,\n",
       "             3.6199],\n",
       "          [  5.2800,   1.7048,  -4.2753,   3.9341,  -1.3720,   5.1133,  -4.5258,\n",
       "             6.0240],\n",
       "          [  0.0952,  -0.1833,   7.6764,  -1.8878,   4.6266,  -0.9071,   0.0241,\n",
       "            -1.9372],\n",
       "          [  3.7228,   7.9129,  -6.5832,   6.3541,  -3.6942,   4.2208,   2.3070,\n",
       "            -1.6370],\n",
       "          [ -5.9752,   1.5262,  -2.4834,   1.4054,  -7.2886,   0.4416,   0.1225,\n",
       "            -1.8685],\n",
       "          [ -2.5396,   3.4102,  -1.2021,   1.1720,   0.7296,   0.8213,  10.1474,\n",
       "             1.8474],\n",
       "          [ -0.6137,  -4.1179,   0.0646,   4.4747,  -0.4499,   9.5888,  -2.5183,\n",
       "            -5.3273]],\n",
       "\n",
       "         [[ -1.2489,  -1.0893,   8.4847,  -1.0904,   3.6712,   6.9259,  -3.6202,\n",
       "             8.8685],\n",
       "          [ -7.5599,  -5.0985,  -5.1248,   2.2981,   0.1744,  -6.4129,   2.2523,\n",
       "            -3.8481],\n",
       "          [  4.6210,  -5.8682,   4.1971,   1.6684,   2.7223,   3.3546,  -0.5590,\n",
       "             7.0851],\n",
       "          [  4.7820,   5.9406,   3.9955,   3.1988,   3.4751,   1.8887,  -9.2231,\n",
       "            -2.7367],\n",
       "          [ -0.5250,   5.7303,   5.5978,   9.2771,   2.8471,   0.4151,  -8.0212,\n",
       "             1.2914],\n",
       "          [  1.7495,   1.4027,   1.6970,   0.0891,   1.2074,   4.4429,   1.1441,\n",
       "            -2.6519],\n",
       "          [ -5.9957,   1.6356,   1.2607,   5.9190,  -7.5994,   4.5038,   7.3015,\n",
       "            -0.3216],\n",
       "          [ -1.8147,   1.0672, -10.6744,   0.6305,  -1.6516,   4.5668,  -3.7244,\n",
       "             2.4485]],\n",
       "\n",
       "         [[  2.5276,   8.1820,  -7.1408,  -0.7410,  -7.3672,   0.5742,  -2.3389,\n",
       "            -5.7475],\n",
       "          [ -5.0144,  -0.4770,   4.1695, -11.5607,  -4.7276,   4.3123,   3.5008,\n",
       "            -1.8475],\n",
       "          [  1.4967,   0.9426,  -3.1709,   4.4125,   0.4389,  -3.3159,  -5.3993,\n",
       "            -4.3383],\n",
       "          [  4.3703,   3.3220,   8.6575,   4.0856,   7.0397,   3.6803,  -8.0119,\n",
       "            -1.7631],\n",
       "          [  5.8835,   2.2541,   2.5373, -12.4686,  10.5646,  -1.9911,   2.8541,\n",
       "             3.0369],\n",
       "          [  9.3080,   0.7189,  -0.4061,   5.3547,   4.2838, -12.6600,  -1.9116,\n",
       "           -10.2533],\n",
       "          [  1.3250,  11.1266,  -4.8665,   4.0605,   0.2316,  -6.4047,   5.0330,\n",
       "            -5.2616],\n",
       "          [ -4.0703,  -4.0169,  -0.9504,  -1.9907,  -2.4265,   0.6058, -10.1534,\n",
       "             0.3430]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q @ k_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_product = (q@k_t)/torch.sqrt(torch.Tensor([k_t.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 8])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4284, -0.1266, -0.1506, -0.0999, -0.9998, -0.7579, -1.0139,\n",
       "           -0.3657],\n",
       "          [ 2.5224,  1.2652,  0.1376,  1.0911, -2.1039, -1.0553,  0.9001,\n",
       "            1.2798],\n",
       "          [ 1.8668,  0.6027, -1.5116,  1.3909, -0.4851,  1.8078, -1.6001,\n",
       "            2.1298],\n",
       "          [ 0.0337, -0.0648,  2.7140, -0.6675,  1.6357, -0.3207,  0.0085,\n",
       "           -0.6849],\n",
       "          [ 1.3162,  2.7976, -2.3275,  2.2465, -1.3061,  1.4923,  0.8157,\n",
       "           -0.5788],\n",
       "          [-2.1125,  0.5396, -0.8780,  0.4969, -2.5769,  0.1561,  0.0433,\n",
       "           -0.6606],\n",
       "          [-0.8979,  1.2057, -0.4250,  0.4144,  0.2579,  0.2904,  3.5876,\n",
       "            0.6531],\n",
       "          [-0.2170, -1.4559,  0.0228,  1.5821, -0.1591,  3.3901, -0.8903,\n",
       "           -1.8835]],\n",
       "\n",
       "         [[-0.4415, -0.3851,  2.9998, -0.3855,  1.2980,  2.4487, -1.2799,\n",
       "            3.1355],\n",
       "          [-2.6728, -1.8026, -1.8119,  0.8125,  0.0617, -2.2673,  0.7963,\n",
       "           -1.3605],\n",
       "          [ 1.6338, -2.0747,  1.4839,  0.5899,  0.9625,  1.1860, -0.1977,\n",
       "            2.5050],\n",
       "          [ 1.6907,  2.1003,  1.4126,  1.1310,  1.2286,  0.6678, -3.2608,\n",
       "           -0.9676],\n",
       "          [-0.1856,  2.0260,  1.9791,  3.2800,  1.0066,  0.1468, -2.8359,\n",
       "            0.4566],\n",
       "          [ 0.6185,  0.4959,  0.6000,  0.0315,  0.4269,  1.5708,  0.4045,\n",
       "           -0.9376],\n",
       "          [-2.1198,  0.5783,  0.4457,  2.0927, -2.6868,  1.5923,  2.5815,\n",
       "           -0.1137],\n",
       "          [-0.6416,  0.3773, -3.7740,  0.2229, -0.5839,  1.6146, -1.3168,\n",
       "            0.8657]],\n",
       "\n",
       "         [[ 0.8936,  2.8928, -2.5247, -0.2620, -2.6047,  0.2030, -0.8269,\n",
       "           -2.0321],\n",
       "          [-1.7729, -0.1686,  1.4741, -4.0873, -1.6714,  1.5246,  1.2377,\n",
       "           -0.6532],\n",
       "          [ 0.5292,  0.3333, -1.1211,  1.5600,  0.1552, -1.1724, -1.9089,\n",
       "           -1.5338],\n",
       "          [ 1.5451,  1.1745,  3.0609,  1.4445,  2.4889,  1.3012, -2.8326,\n",
       "           -0.6233],\n",
       "          [ 2.0801,  0.7969,  0.8971, -4.4083,  3.7351, -0.7040,  1.0091,\n",
       "            1.0737],\n",
       "          [ 3.2909,  0.2542, -0.1436,  1.8932,  1.5145, -4.4760, -0.6758,\n",
       "           -3.6251],\n",
       "          [ 0.4685,  3.9338, -1.7206,  1.4356,  0.0819, -2.2644,  1.7794,\n",
       "           -1.8603],\n",
       "          [-1.4391, -1.4202, -0.3360, -0.7038, -0.8579,  0.2142, -3.5898,\n",
       "            0.1213]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4284, -0.1266, -0.1506, -0.0999, -0.9998, -0.7579, -1.0139,\n",
       "           -0.3657],\n",
       "          [ 2.5224,  1.2652,  0.1376,  1.0911, -2.1039, -1.0553,  0.9001,\n",
       "            1.2798],\n",
       "          [ 1.8668,  0.6027, -1.5116,  1.3909, -0.4851,  1.8078, -1.6001,\n",
       "            2.1298],\n",
       "          [ 0.0337, -0.0648,  2.7140, -0.6675,  1.6357, -0.3207,  0.0085,\n",
       "           -0.6849],\n",
       "          [ 1.3162,  2.7976, -2.3275,  2.2465, -1.3061,  1.4923,  0.8157,\n",
       "           -0.5788],\n",
       "          [-2.1125,  0.5396, -0.8780,  0.4969, -2.5769,  0.1561,  0.0433,\n",
       "           -0.6606],\n",
       "          [-0.8979,  1.2057, -0.4250,  0.4144,  0.2579,  0.2904,  3.5876,\n",
       "            0.6531],\n",
       "          [-0.2170, -1.4559,  0.0228,  1.5821, -0.1591,  3.3901, -0.8903,\n",
       "           -1.8835]],\n",
       "\n",
       "         [[-0.4415, -0.3851,  2.9998, -0.3855,  1.2980,  2.4487, -1.2799,\n",
       "            3.1355],\n",
       "          [-2.6728, -1.8026, -1.8119,  0.8125,  0.0617, -2.2673,  0.7963,\n",
       "           -1.3605],\n",
       "          [ 1.6338, -2.0747,  1.4839,  0.5899,  0.9625,  1.1860, -0.1977,\n",
       "            2.5050],\n",
       "          [ 1.6907,  2.1003,  1.4126,  1.1310,  1.2286,  0.6678, -3.2608,\n",
       "           -0.9676],\n",
       "          [-0.1856,  2.0260,  1.9791,  3.2800,  1.0066,  0.1468, -2.8359,\n",
       "            0.4566],\n",
       "          [ 0.6185,  0.4959,  0.6000,  0.0315,  0.4269,  1.5708,  0.4045,\n",
       "           -0.9376],\n",
       "          [-2.1198,  0.5783,  0.4457,  2.0927, -2.6868,  1.5923,  2.5815,\n",
       "           -0.1137],\n",
       "          [-0.6416,  0.3773, -3.7740,  0.2229, -0.5839,  1.6146, -1.3168,\n",
       "            0.8657]],\n",
       "\n",
       "         [[ 0.8936,  2.8928, -2.5247, -0.2620, -2.6047,  0.2030, -0.8269,\n",
       "           -2.0321],\n",
       "          [-1.7729, -0.1686,  1.4741, -4.0873, -1.6714,  1.5246,  1.2377,\n",
       "           -0.6532],\n",
       "          [ 0.5292,  0.3333, -1.1211,  1.5600,  0.1552, -1.1724, -1.9089,\n",
       "           -1.5338],\n",
       "          [ 1.5451,  1.1745,  3.0609,  1.4445,  2.4889,  1.3012, -2.8326,\n",
       "           -0.6233],\n",
       "          [ 2.0801,  0.7969,  0.8971, -4.4083,  3.7351, -0.7040,  1.0091,\n",
       "            1.0737],\n",
       "          [ 3.2909,  0.2542, -0.1436,  1.8932,  1.5145, -4.4760, -0.6758,\n",
       "           -3.6251],\n",
       "          [ 0.4685,  3.9338, -1.7206,  1.4356,  0.0819, -2.2644,  1.7794,\n",
       "           -1.8603],\n",
       "          [-1.4391, -1.4202, -0.3360, -0.7038, -0.8579,  0.2142, -3.5898,\n",
       "            0.1213]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = nn.Softmax(dim=-1)(scale_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.5269e-01, 1.4506e-01, 1.4161e-01, 1.4898e-01, 6.0578e-02,\n",
       "           7.7155e-02, 5.9727e-02, 1.1421e-01],\n",
       "          [4.6742e-01, 1.3296e-01, 4.3053e-02, 1.1172e-01, 4.5766e-03,\n",
       "           1.3059e-02, 9.2291e-02, 1.3492e-01],\n",
       "          [2.3213e-01, 6.5578e-02, 7.9165e-03, 1.4423e-01, 2.2097e-02,\n",
       "           2.1884e-01, 7.2457e-03, 3.0197e-01],\n",
       "          [4.1459e-02, 3.7572e-02, 6.0490e-01, 2.0565e-02, 2.0578e-01,\n",
       "           2.9088e-02, 4.0430e-02, 2.0209e-02],\n",
       "          [1.0018e-01, 4.4070e-01, 2.6201e-03, 2.5398e-01, 7.2763e-03,\n",
       "           1.1946e-01, 6.0727e-02, 1.5059e-02],\n",
       "          [1.8046e-02, 2.5597e-01, 6.2019e-02, 2.4527e-01, 1.1342e-02,\n",
       "           1.7444e-01, 1.5583e-01, 7.7080e-02],\n",
       "          [8.7404e-03, 7.1631e-02, 1.4025e-02, 3.2466e-02, 2.7765e-02,\n",
       "           2.8680e-02, 7.7547e-01, 4.1222e-02],\n",
       "          [2.1176e-02, 6.1347e-03, 2.6915e-02, 1.2798e-01, 2.2439e-02,\n",
       "           7.8055e-01, 1.0800e-02, 4.0003e-03]],\n",
       "\n",
       "         [[1.0612e-02, 1.1228e-02, 3.3139e-01, 1.1223e-02, 6.0428e-02,\n",
       "           1.9098e-01, 4.5885e-03, 3.7955e-01],\n",
       "          [1.0976e-02, 2.6204e-02, 2.5961e-02, 3.5817e-01, 1.6904e-01,\n",
       "           1.6464e-02, 3.5241e-01, 4.0772e-02],\n",
       "          [1.6842e-01, 4.1288e-03, 1.4498e-01, 5.9297e-02, 8.6072e-02,\n",
       "           1.0763e-01, 2.6979e-02, 4.0249e-01],\n",
       "          [2.0402e-01, 3.0730e-01, 1.5449e-01, 1.1657e-01, 1.2853e-01,\n",
       "           7.3353e-02, 1.4429e-03, 1.4296e-02],\n",
       "          [1.7392e-02, 1.5879e-01, 1.5153e-01, 5.5646e-01, 5.7297e-02,\n",
       "           2.4249e-02, 1.2284e-03, 3.3056e-02],\n",
       "          [1.2727e-01, 1.1258e-01, 1.2493e-01, 7.0756e-02, 1.0507e-01,\n",
       "           3.2981e-01, 1.0274e-01, 2.6847e-02],\n",
       "          [3.9153e-03, 5.8145e-02, 5.0928e-02, 2.6437e-01, 2.2208e-03,\n",
       "           1.6030e-01, 4.3101e-01, 2.9107e-02],\n",
       "          [4.5836e-02, 1.2697e-01, 1.9991e-03, 1.0881e-01, 4.8557e-02,\n",
       "           4.3758e-01, 2.3333e-02, 2.0692e-01]],\n",
       "\n",
       "         [[1.0532e-01, 7.7758e-01, 3.4513e-03, 3.3163e-02, 3.1858e-03,\n",
       "           5.2795e-02, 1.8849e-02, 5.6482e-03],\n",
       "          [1.2005e-02, 5.9711e-02, 3.0868e-01, 1.1863e-03, 1.3286e-02,\n",
       "           3.2467e-01, 2.4369e-01, 3.6780e-02],\n",
       "          [1.6942e-01, 1.3928e-01, 3.2529e-02, 4.7498e-01, 1.1656e-01,\n",
       "           3.0904e-02, 1.4795e-02, 2.1529e-02],\n",
       "          [9.4094e-02, 6.4954e-02, 4.2840e-01, 8.5085e-02, 2.4179e-01,\n",
       "           7.3726e-02, 1.1812e-03, 1.0760e-02],\n",
       "          [1.3178e-01, 3.6524e-02, 4.0371e-02, 2.0043e-04, 6.8965e-01,\n",
       "           8.1424e-03, 4.5155e-02, 4.8170e-02],\n",
       "          [6.5919e-01, 3.1636e-02, 2.1254e-02, 1.6293e-01, 1.1157e-01,\n",
       "           2.7919e-04, 1.2482e-02, 6.5381e-04],\n",
       "          [2.4825e-02, 7.9411e-01, 2.7810e-03, 6.5298e-02, 1.6865e-02,\n",
       "           1.6144e-03, 9.2093e-02, 2.4184e-03],\n",
       "          [5.2611e-02, 5.3614e-02, 1.5854e-01, 1.0975e-01, 9.4076e-02,\n",
       "           2.7484e-01, 6.1241e-03, 2.5045e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.5269e-01, 1.4506e-01, 1.4161e-01, 1.4898e-01, 6.0578e-02,\n",
       "           7.7155e-02, 5.9727e-02, 1.1421e-01],\n",
       "          [4.6742e-01, 1.3296e-01, 4.3053e-02, 1.1172e-01, 4.5766e-03,\n",
       "           1.3059e-02, 9.2291e-02, 1.3492e-01],\n",
       "          [2.3213e-01, 6.5578e-02, 7.9165e-03, 1.4423e-01, 2.2097e-02,\n",
       "           2.1884e-01, 7.2457e-03, 3.0197e-01],\n",
       "          [4.1459e-02, 3.7572e-02, 6.0490e-01, 2.0565e-02, 2.0578e-01,\n",
       "           2.9088e-02, 4.0430e-02, 2.0209e-02],\n",
       "          [1.0018e-01, 4.4070e-01, 2.6201e-03, 2.5398e-01, 7.2763e-03,\n",
       "           1.1946e-01, 6.0727e-02, 1.5059e-02],\n",
       "          [1.8046e-02, 2.5597e-01, 6.2019e-02, 2.4527e-01, 1.1342e-02,\n",
       "           1.7444e-01, 1.5583e-01, 7.7080e-02],\n",
       "          [8.7404e-03, 7.1631e-02, 1.4025e-02, 3.2466e-02, 2.7765e-02,\n",
       "           2.8680e-02, 7.7547e-01, 4.1222e-02],\n",
       "          [2.1176e-02, 6.1347e-03, 2.6915e-02, 1.2798e-01, 2.2439e-02,\n",
       "           7.8055e-01, 1.0800e-02, 4.0003e-03]],\n",
       "\n",
       "         [[1.0612e-02, 1.1228e-02, 3.3139e-01, 1.1223e-02, 6.0428e-02,\n",
       "           1.9098e-01, 4.5885e-03, 3.7955e-01],\n",
       "          [1.0976e-02, 2.6204e-02, 2.5961e-02, 3.5817e-01, 1.6904e-01,\n",
       "           1.6464e-02, 3.5241e-01, 4.0772e-02],\n",
       "          [1.6842e-01, 4.1288e-03, 1.4498e-01, 5.9297e-02, 8.6072e-02,\n",
       "           1.0763e-01, 2.6979e-02, 4.0249e-01],\n",
       "          [2.0402e-01, 3.0730e-01, 1.5449e-01, 1.1657e-01, 1.2853e-01,\n",
       "           7.3353e-02, 1.4429e-03, 1.4296e-02],\n",
       "          [1.7392e-02, 1.5879e-01, 1.5153e-01, 5.5646e-01, 5.7297e-02,\n",
       "           2.4249e-02, 1.2284e-03, 3.3056e-02],\n",
       "          [1.2727e-01, 1.1258e-01, 1.2493e-01, 7.0756e-02, 1.0507e-01,\n",
       "           3.2981e-01, 1.0274e-01, 2.6847e-02],\n",
       "          [3.9153e-03, 5.8145e-02, 5.0928e-02, 2.6437e-01, 2.2208e-03,\n",
       "           1.6030e-01, 4.3101e-01, 2.9107e-02],\n",
       "          [4.5836e-02, 1.2697e-01, 1.9991e-03, 1.0881e-01, 4.8557e-02,\n",
       "           4.3758e-01, 2.3333e-02, 2.0692e-01]],\n",
       "\n",
       "         [[1.0532e-01, 7.7758e-01, 3.4513e-03, 3.3163e-02, 3.1858e-03,\n",
       "           5.2795e-02, 1.8849e-02, 5.6482e-03],\n",
       "          [1.2005e-02, 5.9711e-02, 3.0868e-01, 1.1863e-03, 1.3286e-02,\n",
       "           3.2467e-01, 2.4369e-01, 3.6780e-02],\n",
       "          [1.6942e-01, 1.3928e-01, 3.2529e-02, 4.7498e-01, 1.1656e-01,\n",
       "           3.0904e-02, 1.4795e-02, 2.1529e-02],\n",
       "          [9.4094e-02, 6.4954e-02, 4.2840e-01, 8.5085e-02, 2.4179e-01,\n",
       "           7.3726e-02, 1.1812e-03, 1.0760e-02],\n",
       "          [1.3178e-01, 3.6524e-02, 4.0371e-02, 2.0043e-04, 6.8965e-01,\n",
       "           8.1424e-03, 4.5155e-02, 4.8170e-02],\n",
       "          [6.5919e-01, 3.1636e-02, 2.1254e-02, 1.6293e-01, 1.1157e-01,\n",
       "           2.7919e-04, 1.2482e-02, 6.5381e-04],\n",
       "          [2.4825e-02, 7.9411e-01, 2.7810e-03, 6.5298e-02, 1.6865e-02,\n",
       "           1.6144e-03, 9.2093e-02, 2.4184e-03],\n",
       "          [5.2611e-02, 5.3614e-02, 1.5854e-01, 1.0975e-01, 9.4076e-02,\n",
       "           2.7484e-01, 6.1241e-03, 2.5045e-01]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 8])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = attention_output @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 64])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_output = layer_output.view(2,3,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_concat = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = w_concat(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.mean(-1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0712],\n",
       "         [0.0562],\n",
       "         [0.0983]],\n",
       "\n",
       "        [[0.0712],\n",
       "         [0.0562],\n",
       "         [0.0983]]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.var(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0711],\n",
       "         [0.0561],\n",
       "         [0.0981]],\n",
       "\n",
       "        [[0.0711],\n",
       "         [0.0561],\n",
       "         [0.0981]]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.var(-1, keepdim=True, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = nn.Parameter(torch.ones(d_model))\n",
    "beta = nn.Parameter(torch.zeros(d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand((2,3,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8900, 0.9574, 0.4480,  ..., 0.6815, 0.9838, 0.6955],\n",
       "         [0.4489, 0.2031, 0.7776,  ..., 0.2069, 0.8618, 0.3260],\n",
       "         [0.2983, 0.6056, 0.7801,  ..., 0.9291, 0.3297, 0.6644]],\n",
       "\n",
       "        [[0.1237, 0.2730, 0.5457,  ..., 0.6981, 0.4524, 0.2758],\n",
       "         [0.4233, 0.9857, 0.5810,  ..., 0.5019, 0.9301, 0.6828],\n",
       "         [0.6560, 0.5124, 0.4303,  ..., 0.6155, 0.7147, 0.4626]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8900, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4489, 0.2031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2983, 0.6056, 0.7801,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1237, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4233, 0.9857, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6560, 0.5124, 0.4303,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(t).masked_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
