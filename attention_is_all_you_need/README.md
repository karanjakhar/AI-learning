This contains code and reading material related to Attention is all you need. Which introduces concept of Attention and transformers architecture.


- Dictionary, to convert text to number
- Positional embedding
- attention layers
- multihead attention layers


![transformer Architecture](./assets/transformer_arch.png)
![attention formula](assets/attention_formula.png)
![multihead attention formula](assets/multihead_attention_formula.png)
![multihead attention architecture](assets/multihead_n_attention_architecture.png)
![optimizer](assets/optimizer.png)
![positional encoding formula](assets/positional_encoding_formula.png)
![positionwise feed forward formula](assets/positionwise_feed_forward_formula.png)
![positionwise feed forward](assets/positionwise_feed_forward.jpg)
