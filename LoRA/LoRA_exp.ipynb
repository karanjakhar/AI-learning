{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA implementation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn \n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch determinitic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting MNIST dataset for training our simple model\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the devie\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network to classify the digits, make the model little big to see the use of LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, hidden_size1 = 1000, hidden_size_2 = 2000):\n",
    "        super(ClassifyNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "net = ClassifyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epochs = 5, total_iterations_limit = None):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    total_iterations = 0 \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iteration = tqdm(train_loader, desc = f'Epoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        if total_iterations_limit:\n",
    "            data_iteration.total = total_iterations_limit\n",
    "        \n",
    "        for data in data_iteration:\n",
    "\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x)\n",
    "            loss_value = loss_fn(output, y)\n",
    "            loss_sum += loss_value.item()\n",
    "\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "\n",
    "            data_iteration.set_postfix(loss = avg_loss)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 6000/6000 [00:31<00:00, 192.44it/s, loss=0.236]\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, net, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep a copy of original weights (clone them), so later we can confirm that fine tuning with LoRA doesn't alter the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    original_weights[name] = param.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear1.weight': tensor([[ 0.0015,  0.0210, -0.0276,  ...,  0.0237,  0.0055,  0.0039],\n",
       "         [ 0.0078,  0.0126,  0.0172,  ...,  0.0073,  0.0216, -0.0023],\n",
       "         [ 0.0124,  0.0475, -0.0007,  ...,  0.0122,  0.0337,  0.0406],\n",
       "         ...,\n",
       "         [-0.0175,  0.0460,  0.0443,  ...,  0.0133,  0.0394, -0.0132],\n",
       "         [ 0.0716,  0.0349,  0.0206,  ...,  0.0549,  0.0485,  0.0493],\n",
       "         [ 0.0216, -0.0021,  0.0491,  ...,  0.0584,  0.0319,  0.0423]],\n",
       "        device='cuda:0'),\n",
       " 'linear1.bias': tensor([-2.6380e-02, -6.2766e-03, -4.0181e-02, -1.0200e-02,  6.3419e-03,\n",
       "         -4.9645e-02, -4.1591e-02, -5.0680e-03, -5.5057e-02, -7.0120e-03,\n",
       "          4.5819e-03, -1.1517e-02, -2.7861e-02, -4.0841e-02,  4.1142e-03,\n",
       "         -5.0162e-02, -4.8437e-02, -4.1003e-02, -1.7643e-02, -5.5479e-02,\n",
       "         -3.2971e-02, -4.6315e-02,  1.0228e-02, -2.2976e-02, -1.4506e-02,\n",
       "         -4.3609e-02, -1.1930e-03, -2.7267e-02,  8.9650e-03, -2.3131e-02,\n",
       "          2.4631e-03, -4.0215e-02, -2.1087e-02, -8.7772e-03, -4.4958e-02,\n",
       "         -3.7298e-03, -6.0452e-02,  2.5298e-03, -4.2388e-02, -4.9511e-03,\n",
       "         -6.3125e-03, -3.5696e-02,  2.0458e-02,  1.2561e-03, -2.0519e-02,\n",
       "          1.4365e-02, -5.7749e-02, -2.1512e-02, -3.5678e-02, -4.1708e-02,\n",
       "          1.4492e-02, -3.4023e-02,  1.8052e-03, -2.3906e-02, -3.6084e-02,\n",
       "         -4.5688e-02, -3.2438e-02, -5.2949e-02, -3.3284e-02, -1.2185e-02,\n",
       "         -2.9986e-02, -1.9608e-02,  2.4916e-02, -4.6038e-03, -4.9405e-02,\n",
       "         -4.8360e-02,  1.4645e-02, -4.7139e-02, -4.5118e-02, -6.1098e-02,\n",
       "         -4.7631e-02,  3.1465e-03, -6.0279e-02, -6.0564e-03, -9.2556e-03,\n",
       "         -1.4276e-02,  6.8335e-03,  2.6052e-03, -7.0302e-02, -5.6727e-03,\n",
       "          2.4199e-04, -6.0258e-02,  2.2466e-02, -1.2716e-02, -3.7280e-02,\n",
       "         -5.5554e-02, -3.9930e-03, -4.2513e-02,  4.6507e-03, -2.3457e-02,\n",
       "         -1.6852e-02, -1.1789e-02, -1.5643e-02, -3.1408e-02, -4.1344e-02,\n",
       "         -2.8582e-02, -3.8632e-02, -1.3682e-02, -5.4215e-02, -5.5922e-02,\n",
       "         -5.1596e-02,  2.1745e-02, -1.6354e-02,  1.4661e-02, -6.5621e-02,\n",
       "          1.1025e-02,  8.0442e-03, -1.6294e-02, -6.8255e-02, -7.7687e-02,\n",
       "         -1.5671e-02, -3.1462e-02, -3.2851e-02, -3.3733e-02, -4.9635e-02,\n",
       "         -5.1066e-03,  1.1019e-02,  1.5995e-02, -4.0125e-02, -1.2507e-02,\n",
       "         -2.9093e-03, -6.8000e-02, -3.1127e-02, -3.0709e-02, -1.5853e-02,\n",
       "         -1.2714e-02, -7.4783e-03, -5.2290e-02, -1.2767e-02, -3.4626e-02,\n",
       "          5.4824e-03, -2.2730e-02, -3.0947e-02,  2.0764e-02, -4.7123e-02,\n",
       "         -1.8814e-03, -4.3166e-02, -1.4609e-02, -3.8599e-02,  3.8301e-03,\n",
       "         -2.9478e-02,  8.4454e-04, -2.4394e-02, -6.4069e-03, -5.5250e-02,\n",
       "          2.7300e-03, -2.0019e-02, -1.9235e-03, -2.0024e-02, -5.5094e-02,\n",
       "         -5.9991e-03, -7.4200e-03, -5.8569e-02, -6.4978e-02, -4.1081e-02,\n",
       "         -6.5801e-03, -6.0822e-02, -2.1664e-02, -3.8586e-02, -1.7254e-02,\n",
       "          1.6607e-02, -1.9762e-02,  1.9618e-02,  6.8632e-03, -4.0498e-02,\n",
       "         -2.8004e-02, -1.5093e-02, -5.9702e-02, -2.5767e-02, -4.8257e-03,\n",
       "         -3.1779e-02, -7.7552e-02, -2.3480e-02,  1.9260e-02, -1.9157e-02,\n",
       "         -4.1342e-02, -2.5422e-03, -2.4360e-02, -3.1927e-02, -3.8750e-02,\n",
       "          1.1482e-02, -5.7327e-02, -3.7941e-02, -5.5967e-02,  2.6655e-02,\n",
       "         -6.2160e-02,  1.2791e-02, -5.3576e-02, -9.2773e-03, -2.5107e-02,\n",
       "         -4.4884e-02,  6.7693e-03, -4.1218e-02, -4.9358e-02,  3.4989e-03,\n",
       "         -2.2232e-02, -2.2441e-04, -4.6423e-03, -3.3903e-02, -5.5422e-02,\n",
       "          9.5185e-03, -1.0088e-02, -6.8694e-02, -2.5608e-02, -1.8742e-02,\n",
       "          3.2932e-03, -2.1959e-02,  1.7714e-02, -3.9958e-02, -2.2475e-02,\n",
       "         -8.4659e-04, -3.9549e-02, -1.3214e-02, -1.4625e-02, -8.9010e-03,\n",
       "         -3.4493e-02, -2.5392e-02, -1.4443e-02,  6.1236e-03, -2.3574e-02,\n",
       "         -8.1340e-02, -1.7441e-02,  1.0996e-02, -6.3781e-03, -3.7235e-02,\n",
       "         -1.4907e-02, -3.6183e-02, -1.5397e-02, -1.5301e-02, -1.1651e-02,\n",
       "         -5.9686e-02, -2.1519e-02,  6.8466e-04, -4.1972e-02, -2.6990e-02,\n",
       "         -3.3656e-02, -4.9715e-02,  1.1074e-02,  7.5091e-04, -7.1497e-03,\n",
       "         -3.8354e-02, -3.1461e-02, -5.0282e-02, -7.3585e-02, -3.0780e-02,\n",
       "         -6.2604e-02, -2.4460e-03, -2.1081e-02, -6.8718e-02, -1.8607e-02,\n",
       "         -1.3357e-02, -1.8135e-02,  1.8465e-03, -5.6804e-02,  1.6283e-02,\n",
       "         -3.7772e-02, -2.1093e-02, -3.1036e-03, -4.2947e-02, -4.7162e-02,\n",
       "         -4.2938e-02,  5.0072e-03, -3.6677e-02, -3.9633e-02, -2.8142e-02,\n",
       "         -6.6320e-02, -2.7378e-02,  7.3499e-03, -1.3250e-02, -6.8530e-04,\n",
       "         -4.5649e-02,  2.0861e-02, -6.4528e-02, -1.2812e-02,  1.7471e-03,\n",
       "         -3.9708e-02, -4.6174e-03, -3.3422e-02, -4.8599e-02,  6.6083e-03,\n",
       "         -5.8973e-04, -1.3556e-02, -2.7302e-02, -5.3911e-02, -1.5746e-02,\n",
       "         -3.0699e-02, -5.2360e-02, -1.5505e-02,  7.8629e-03, -4.6259e-02,\n",
       "         -5.5219e-02, -2.6782e-02,  2.1260e-03,  1.4163e-04, -1.9159e-02,\n",
       "         -2.4128e-02,  4.5138e-03, -1.1084e-02, -4.8069e-02,  4.6096e-04,\n",
       "         -2.5949e-02, -3.1733e-02, -5.6309e-02,  4.4689e-04, -4.8821e-02,\n",
       "          9.3972e-03, -2.9694e-02, -1.9314e-03, -4.9754e-02, -1.1888e-02,\n",
       "         -2.8790e-02,  1.1856e-02, -8.7800e-03, -6.5210e-02,  2.5572e-02,\n",
       "         -7.2550e-03, -4.5568e-02,  4.8394e-03, -1.0564e-02, -3.3145e-02,\n",
       "          1.6019e-02, -3.3212e-02,  4.7072e-03, -4.5811e-03, -6.1459e-02,\n",
       "         -1.7700e-02, -4.0514e-03, -6.0485e-02, -4.1822e-02, -3.2637e-02,\n",
       "          1.1578e-02, -7.7509e-03, -5.9295e-03, -2.9015e-02, -5.5662e-02,\n",
       "         -3.6098e-02, -3.8461e-02, -6.2852e-02,  1.2429e-02, -1.2033e-02,\n",
       "         -9.0525e-04, -4.9636e-02, -1.6737e-02, -6.0764e-02, -3.3050e-02,\n",
       "         -1.8827e-02, -5.0088e-02, -6.6922e-02, -2.2150e-02, -5.3724e-03,\n",
       "         -1.8896e-02, -4.1103e-03, -4.5863e-02, -3.2723e-02, -7.8210e-02,\n",
       "          1.0132e-02, -4.1409e-02, -4.1575e-02,  1.1545e-02,  3.0395e-03,\n",
       "         -6.9532e-03, -3.7677e-03, -2.7117e-02, -2.8935e-02,  6.1676e-03,\n",
       "          2.5803e-03, -4.5260e-02, -1.5349e-03, -1.6629e-02,  2.0243e-02,\n",
       "         -1.0166e-02,  2.2765e-02,  2.4533e-02, -3.9168e-02,  5.7594e-03,\n",
       "         -3.1766e-02, -2.2253e-02, -3.5880e-02,  1.9151e-02, -5.6141e-02,\n",
       "         -3.7267e-02, -1.3177e-02, -5.1560e-02,  1.9799e-02, -3.1246e-02,\n",
       "         -4.1300e-02, -4.9411e-03,  2.3468e-03, -4.5110e-02, -6.5359e-03,\n",
       "         -4.3726e-02, -3.3637e-02, -1.4951e-02, -9.1522e-03, -4.3849e-02,\n",
       "         -2.5260e-02, -5.5301e-02, -3.0542e-02, -3.0855e-02, -2.0060e-02,\n",
       "         -6.3209e-02,  1.0779e-02, -6.5227e-02,  7.0230e-03, -6.3885e-02,\n",
       "          1.4395e-02, -1.6284e-02, -1.3704e-03, -2.3890e-02, -4.8628e-02,\n",
       "         -2.2594e-02, -1.4673e-02, -3.5870e-02, -2.1800e-02, -8.0427e-03,\n",
       "         -4.0254e-02, -3.1075e-02, -8.2824e-03, -6.5809e-02,  3.9573e-03,\n",
       "         -7.3552e-02, -3.3553e-02, -9.2658e-02, -1.3199e-03, -1.8317e-02,\n",
       "         -5.2646e-02,  1.4885e-02,  2.3756e-02,  1.9902e-02, -1.2263e-02,\n",
       "         -2.3142e-02, -4.3614e-02,  9.8333e-03, -4.3946e-02, -5.8310e-02,\n",
       "         -3.8342e-02, -4.0273e-02, -4.6403e-02, -4.8185e-02, -1.7659e-02,\n",
       "          6.9771e-03, -9.3657e-03, -7.8428e-02, -2.7292e-02, -5.4673e-02,\n",
       "          1.6121e-02, -2.3246e-02, -7.0235e-02, -3.5216e-03, -4.5655e-03,\n",
       "         -3.9941e-02, -6.4786e-03,  5.8356e-03, -3.9851e-02, -4.6358e-02,\n",
       "         -1.7906e-02,  1.0503e-03, -2.0630e-02, -2.1091e-02, -3.7628e-02,\n",
       "         -2.1210e-02, -1.0269e-02,  4.0369e-03, -3.6956e-02, -3.5924e-02,\n",
       "         -6.7494e-02, -3.6568e-02, -2.3733e-02,  1.2337e-02, -5.0782e-02,\n",
       "          8.7429e-03, -2.3491e-02,  2.4247e-03, -4.8290e-02, -1.4330e-02,\n",
       "         -5.9666e-02, -2.8235e-02, -2.6227e-02, -1.4109e-03, -2.0099e-03,\n",
       "         -4.6563e-02, -2.7451e-02, -4.8993e-02, -2.4630e-02,  1.6312e-02,\n",
       "         -1.1705e-02, -2.3492e-02, -1.7328e-02, -4.6273e-02, -2.1796e-02,\n",
       "         -5.4969e-02,  1.9780e-04, -2.1043e-02, -2.5288e-02, -9.3501e-03,\n",
       "         -5.7280e-02, -4.2869e-02, -2.4861e-02, -3.6897e-02, -4.8652e-02,\n",
       "         -1.5514e-02, -9.7941e-03, -5.0896e-02, -2.6364e-02, -8.3676e-03,\n",
       "         -3.3881e-02, -3.9196e-02, -5.4006e-02, -4.3138e-02, -4.4507e-02,\n",
       "         -4.5909e-02,  3.4083e-03, -1.7064e-02, -3.8095e-02, -4.6522e-02,\n",
       "          1.4121e-02, -2.7245e-02,  8.4502e-03, -2.2621e-02, -9.2603e-03,\n",
       "          1.9032e-02, -3.3663e-02, -3.9573e-02, -2.8076e-02, -1.1845e-01,\n",
       "         -4.1534e-02, -3.2700e-03, -4.3512e-02,  6.4717e-03, -2.3634e-02,\n",
       "         -7.8845e-02, -1.8518e-02, -7.0693e-02,  1.0999e-02, -3.3329e-03,\n",
       "         -2.8721e-02, -2.0117e-02, -3.9993e-02, -3.2754e-02, -6.9316e-02,\n",
       "         -3.9167e-02, -3.3965e-03, -4.2276e-02, -4.4506e-02, -6.4722e-02,\n",
       "         -3.5430e-02, -4.3001e-02, -3.5187e-02,  1.9568e-02,  2.7186e-02,\n",
       "         -2.8831e-02, -3.1862e-02, -3.1268e-02, -3.2590e-04,  1.3535e-02,\n",
       "          2.5338e-03, -4.6041e-02, -3.4325e-02, -3.9860e-02, -2.5325e-02,\n",
       "         -3.6145e-02, -5.1011e-02, -6.4687e-02, -1.9692e-03, -1.6936e-02,\n",
       "         -7.3235e-02, -6.3825e-03, -1.4219e-02, -2.8941e-02, -3.4690e-03,\n",
       "         -3.5911e-02, -4.4034e-02, -3.7987e-02, -2.0206e-02, -6.9956e-02,\n",
       "         -7.8053e-03, -2.1472e-02, -1.5516e-02, -4.7850e-02, -7.7583e-02,\n",
       "         -2.8107e-02, -5.1004e-02, -2.3030e-02, -1.6935e-02, -6.0022e-02,\n",
       "         -5.5858e-02, -4.7999e-02, -3.0967e-02, -8.2205e-04,  1.9333e-02,\n",
       "         -2.9722e-02, -2.7487e-02, -6.9628e-02,  3.6554e-03, -6.3886e-02,\n",
       "         -2.5405e-02, -3.3387e-02, -3.9260e-02, -4.2764e-02, -4.6702e-02,\n",
       "         -2.8658e-02,  1.7870e-02, -2.0017e-02,  1.9578e-02, -2.8319e-02,\n",
       "         -1.2660e-02, -1.4961e-02,  6.3118e-03,  5.2613e-03, -1.7047e-02,\n",
       "         -6.0876e-02, -4.9889e-02, -1.0514e-02, -3.5377e-02, -4.1342e-02,\n",
       "         -5.8387e-03, -3.5005e-02, -4.1985e-02, -2.5543e-02, -2.8932e-02,\n",
       "         -5.1057e-03, -2.3635e-02, -5.6376e-02,  4.0530e-03, -5.8484e-02,\n",
       "          3.1366e-03, -1.3473e-02, -2.0222e-03, -8.5725e-02, -3.5934e-03,\n",
       "         -2.2175e-02, -5.8127e-03,  2.3631e-02, -6.5912e-02, -2.8272e-02,\n",
       "         -6.9354e-02,  7.7244e-03, -8.0607e-02, -5.0629e-03, -2.3962e-02,\n",
       "          1.5325e-02, -4.4480e-03,  1.1288e-02, -7.4119e-02,  4.3720e-03,\n",
       "          2.5345e-03, -6.7233e-02, -4.4506e-02, -1.6691e-02, -4.1359e-02,\n",
       "         -8.3865e-02, -6.6128e-02, -4.8270e-02,  3.1858e-03,  1.3003e-02,\n",
       "         -6.0051e-02, -6.9864e-02, -5.4777e-02,  1.6307e-02,  3.3940e-02,\n",
       "         -8.3683e-02, -2.5372e-02, -5.2553e-02, -2.3554e-02, -5.8292e-02,\n",
       "         -5.6763e-03, -3.1320e-02, -5.5673e-02, -2.3702e-02, -4.0948e-02,\n",
       "         -9.9388e-02, -2.7477e-02, -3.6338e-02, -6.3710e-02, -8.4879e-05,\n",
       "         -6.6453e-03, -1.8706e-02, -2.8823e-02, -1.8191e-02, -3.7554e-02,\n",
       "         -3.3292e-02, -6.3991e-02, -2.0313e-02,  3.0600e-03, -3.8252e-02,\n",
       "         -1.1744e-03, -3.0701e-02, -1.9703e-02, -7.5931e-03, -7.6921e-02,\n",
       "         -2.8046e-02, -5.3455e-02, -3.6975e-02, -1.1479e-02, -4.0229e-02,\n",
       "         -4.2753e-02,  1.8069e-02, -4.7355e-03, -6.2389e-02, -3.8683e-02,\n",
       "         -3.7420e-02, -2.0838e-02, -4.3973e-02, -1.9670e-02, -7.3673e-03,\n",
       "          1.6687e-02, -1.0112e-02, -4.4221e-02,  1.6930e-02,  2.0444e-02,\n",
       "         -3.3785e-02, -5.7679e-02,  1.5512e-02, -7.0307e-03, -2.3462e-02,\n",
       "         -4.7587e-02, -4.1176e-02, -3.1114e-03,  9.2745e-03, -4.8084e-02,\n",
       "         -7.4330e-02, -5.0155e-02, -4.1105e-02, -3.8571e-02, -1.4512e-02,\n",
       "         -3.8023e-02, -4.9963e-02, -2.4482e-02, -5.5295e-02, -3.0691e-02,\n",
       "         -5.0593e-02, -3.9754e-02, -2.4902e-02, -3.3676e-02, -3.4983e-02,\n",
       "         -4.9381e-02, -1.3196e-02, -4.0800e-02, -4.6572e-02, -1.9142e-02,\n",
       "         -3.4181e-02, -7.6935e-03, -5.9516e-02, -4.3135e-02,  1.8250e-02,\n",
       "          3.8924e-03, -3.6394e-02, -3.2610e-02, -4.3139e-02,  1.4458e-02,\n",
       "         -7.5823e-03, -6.4857e-03, -3.3056e-02, -2.2934e-02,  1.8321e-02,\n",
       "          1.1604e-02, -3.2245e-02, -1.5855e-02, -9.2584e-02, -1.9131e-04,\n",
       "         -3.8431e-02, -1.3192e-02, -2.4396e-02,  2.5414e-03,  8.3043e-03,\n",
       "         -5.3287e-02, -2.0336e-02, -3.7001e-02, -3.0201e-02, -6.4713e-02,\n",
       "         -1.7829e-03, -7.8098e-03,  1.3874e-02, -5.0785e-02, -1.7765e-02,\n",
       "         -4.0572e-02, -7.1644e-02, -2.8985e-02, -3.9656e-02, -1.2792e-03,\n",
       "         -3.4030e-02, -2.0137e-02, -1.1950e-02, -5.5781e-03, -5.1455e-02,\n",
       "         -1.2193e-02, -1.0443e-02,  4.8935e-03, -1.5000e-02, -2.3355e-02,\n",
       "         -1.7791e-02, -5.7952e-02,  1.0022e-02, -4.4851e-02, -2.0979e-02,\n",
       "         -4.0149e-02, -3.7980e-02, -4.4268e-03,  1.3392e-03, -2.0864e-02,\n",
       "         -2.7774e-02, -2.3098e-02, -7.5932e-02,  1.0437e-02,  9.6967e-03,\n",
       "          1.0169e-02, -3.5348e-02, -5.0574e-02, -3.8393e-03,  8.0521e-03,\n",
       "         -4.0006e-02, -4.7438e-02,  8.5174e-03, -2.9507e-02, -5.0143e-03,\n",
       "          1.7125e-02, -4.6189e-02, -5.1275e-02, -4.3798e-02, -6.0570e-02,\n",
       "         -1.0526e-02, -3.8358e-02, -3.1338e-02, -5.4659e-02, -2.7204e-02,\n",
       "         -3.3941e-02, -3.6422e-02, -1.9946e-02, -1.6567e-02, -1.7836e-02,\n",
       "         -3.4277e-02, -7.9863e-03, -1.3846e-02, -4.0158e-02, -2.8452e-02,\n",
       "         -6.9557e-03, -1.8822e-02, -5.3198e-02, -2.0206e-02, -7.1762e-02,\n",
       "          2.2669e-02, -6.4696e-02, -4.7188e-02, -1.3512e-02, -4.8518e-02,\n",
       "         -2.7317e-02, -2.9740e-02, -1.0366e-02, -5.2780e-02,  3.9485e-02,\n",
       "         -4.9322e-02,  2.4599e-03, -3.9264e-02, -1.8261e-03, -6.9902e-03,\n",
       "         -2.0474e-02, -7.0926e-02, -4.9080e-02, -6.3025e-03,  2.8386e-03,\n",
       "         -2.5572e-03, -2.9725e-02, -5.3059e-02, -2.7123e-02, -5.6565e-02,\n",
       "         -4.2463e-02, -6.5871e-02, -5.0701e-02, -4.8484e-02, -1.9116e-02,\n",
       "         -4.0786e-02, -7.0839e-02, -6.4078e-02, -3.3372e-02, -5.2735e-02,\n",
       "         -3.8940e-02, -6.1573e-03,  1.1402e-02, -1.6273e-02, -3.9705e-02,\n",
       "          2.5676e-02, -2.1993e-02, -3.1848e-02,  2.2711e-02, -4.5531e-02,\n",
       "         -4.8129e-02, -5.3011e-02, -5.9603e-02, -6.8737e-02, -6.7772e-04,\n",
       "         -4.1262e-02,  2.0664e-02, -4.9201e-02, -4.0102e-02,  5.5796e-03,\n",
       "          3.1615e-03,  1.1412e-02,  2.8396e-02, -8.0506e-02, -2.0573e-03,\n",
       "         -5.6627e-02, -5.9604e-03, -3.2260e-02, -1.1105e-02, -3.1706e-02,\n",
       "         -2.6151e-02, -2.1349e-02,  2.2660e-03, -7.0434e-02,  1.1057e-02,\n",
       "         -5.2231e-02, -6.5201e-02, -1.9879e-02,  3.6236e-03, -5.2287e-02,\n",
       "          1.3738e-02, -5.6485e-02, -5.0785e-02, -2.1180e-02, -3.0320e-02,\n",
       "         -1.9542e-03,  1.0913e-02,  1.8688e-02, -3.3644e-02, -5.7559e-02,\n",
       "         -5.4714e-02, -1.9755e-02, -5.5909e-02, -2.8793e-02, -2.1555e-02,\n",
       "         -2.5386e-02, -2.6274e-02, -9.0640e-03, -5.8079e-02, -2.0189e-02,\n",
       "         -5.6524e-02,  1.4220e-02, -2.9044e-02, -2.7379e-02, -4.5943e-02,\n",
       "         -9.0214e-02, -1.6323e-02, -3.3329e-02, -5.1458e-02, -6.0337e-02,\n",
       "         -4.0383e-02, -3.0929e-02, -3.4201e-02, -3.5635e-02, -3.9070e-02,\n",
       "         -4.9599e-02,  1.3004e-02, -1.7575e-02, -2.8194e-03, -3.0911e-02,\n",
       "         -2.8570e-02, -6.3000e-02, -3.6666e-02, -6.6955e-03, -6.1998e-02,\n",
       "         -5.5226e-02, -4.1020e-02,  2.2023e-02, -4.7811e-02, -3.9616e-02,\n",
       "          1.4550e-02,  1.7922e-02, -2.9180e-02, -5.1561e-02, -4.3574e-02,\n",
       "         -4.8800e-02, -2.8165e-02, -7.0955e-02, -3.2366e-02,  5.7569e-03,\n",
       "         -3.3900e-02, -3.4809e-02, -2.2807e-02, -8.0293e-02, -3.9222e-02,\n",
       "         -3.5903e-02, -4.6959e-02, -3.3434e-02, -9.0129e-03, -4.5032e-02,\n",
       "         -3.8703e-02, -2.5232e-02, -4.4008e-02,  5.4014e-04, -2.9095e-02,\n",
       "          2.8260e-03, -1.1100e-02, -3.9009e-02, -1.2963e-02, -4.8644e-02,\n",
       "         -1.5252e-02, -5.0891e-02,  7.5150e-03, -4.5843e-02, -2.2432e-02],\n",
       "        device='cuda:0'),\n",
       " 'linear2.weight': tensor([[-0.0915,  0.0366, -0.0157,  ...,  0.0511, -0.0124, -0.0165],\n",
       "         [-0.0464,  0.0274,  0.0084,  ...,  0.0218,  0.0092, -0.0362],\n",
       "         [ 0.0650,  0.0018, -0.0533,  ..., -0.0037,  0.0726, -0.0081],\n",
       "         ...,\n",
       "         [ 0.0118, -0.0100,  0.0277,  ..., -0.0394, -0.0389, -0.0066],\n",
       "         [-0.1903, -0.0420,  0.0446,  ..., -0.0435, -0.0350,  0.0005],\n",
       "         [-0.0434,  0.0145, -0.0057,  ..., -0.0188, -0.0080, -0.0234]],\n",
       "        device='cuda:0'),\n",
       " 'linear2.bias': tensor([-0.0226, -0.0355, -0.0192,  ..., -0.0094, -0.0538, -0.0618],\n",
       "        device='cuda:0'),\n",
       " 'linear3.weight': tensor([[-0.0333,  0.0161,  0.0193,  ...,  0.0003, -0.2165, -0.0126],\n",
       "         [-0.0273, -0.0007, -0.1004,  ...,  0.0235, -0.0317, -0.0020],\n",
       "         [ 0.0276,  0.0012, -0.0488,  ...,  0.0111, -0.1212, -0.0447],\n",
       "         ...,\n",
       "         [-0.0176,  0.0055, -0.0271,  ...,  0.0089, -0.0879, -0.0242],\n",
       "         [ 0.0477,  0.0134,  0.0436,  ...,  0.0072,  0.0537, -0.0207],\n",
       "         [-0.0362,  0.0075, -0.0368,  ..., -0.0044, -0.0977, -0.0007]],\n",
       "        device='cuda:0'),\n",
       " 'linear3.bias': tensor([ 0.0011, -0.0125, -0.0366, -0.0389, -0.0488, -0.0218, -0.0361, -0.0472,\n",
       "          0.1550,  0.0597], device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            for idx, i in enumerate(outputs):\n",
    "                if torch.argmax(i) == labels[idx]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong_counts[torch.argmax(i)] += 1\n",
    "                total += 1\n",
    "\n",
    "    print(f'Accuracy: {round(correct/total, 4)}')\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f'Wrong count for {i}: {wrong_counts[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:02<00:00, 377.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9663\n",
      "Wrong count for 0: 13\n",
      "Wrong count for 1: 35\n",
      "Wrong count for 2: 36\n",
      "Wrong count for 3: 30\n",
      "Wrong count for 4: 43\n",
      "Wrong count for 5: 20\n",
      "Wrong count for 6: 16\n",
      "Wrong count for 7: 29\n",
      "Wrong count for 8: 90\n",
      "Wrong count for 9: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check how many parameters are in the original network, before introducing the LoRA matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "total_parameters_original = 0 \n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f\"Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}\")\n",
    "\n",
    "print(f\"Total parameters: {total_parameters_original:,}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA parameterization as described in the paper.\n",
    "\n",
    "it uses PyTorch parameterization: https://pytorch.org/tutorials/intermediate/parametrizations.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParameterization(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out, rank=1, alpha=1, device='cpu'):\n",
    "        super(LoRAParameterization, self).__init__()\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank, feature_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((feature_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "\n",
    "        self.scale = alpha / rank \n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the parameterization to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parametrize \n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "\n",
    "    feature_in, feature_out = layer.weight.shape\n",
    "\n",
    "    return LoRAParameterization(feature_in, feature_out, rank=rank, alpha=lora_alpha, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=2000, out_features=10, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRAParameterization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrize.register_parametrization(net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device))\n",
    "parametrize.register_parametrization(net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device))\n",
    "parametrize.register_parametrization(net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifyNet(\n",
       "  (linear1): ParametrizedLinear(\n",
       "    in_features=784, out_features=1000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear2): ParametrizedLinear(\n",
       "    in_features=1000, out_features=2000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear3): ParametrizedLinear(\n",
       "    in_features=2000, out_features=10, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of ClassifyNet(\n",
       "  (linear1): ParametrizedLinear(\n",
       "    in_features=784, out_features=1000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear2): ParametrizedLinear(\n",
       "    in_features=1000, out_features=2000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear3): ParametrizedLinear(\n",
       "    in_features=2000, out_features=10, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Paramters introduced by LoRA: 6,794\n",
      "Parameter increment: 0.24%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations['weight'][0].lora_A.nelement() + layer.parametrizations['weight'][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}')\n",
    "    \n",
    "# Non-LoRA parameters count must match the original network\n",
    "assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_original:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Paramters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_increment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameter increment: {parameters_increment:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.linear1.parametrizations['weight'][0].lora_A.nelement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine tune the model on the digit 4 and only for 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "LoRA paramerter linear1.parametrizations.weight.0.lora_A\n",
      "LoRA paramerter linear1.parametrizations.weight.0.lora_B\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "LoRA paramerter linear2.parametrizations.weight.0.lora_A\n",
      "LoRA paramerter linear2.parametrizations.weight.0.lora_B\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n",
      "LoRA paramerter linear3.parametrizations.weight.0.lora_A\n",
      "LoRA paramerter linear3.parametrizations.weight.0.lora_B\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(f'LoRA paramerter {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training just on digit 4 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dadtaset again, by keeping only the digit 4\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "exclude_indices = mnist_trainset.targets!= 4\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  99%|█████████▉| 99/100 [00:00<00:00, 147.70it/s, loss=0.117]\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, net, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the fine tuning did alter the original weights, but only the ones introduced by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
    "        layer.parametrizations['weight'][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0015,  0.0210, -0.0276,  ...,  0.0237,  0.0055,  0.0039],\n",
       "        [ 0.0078,  0.0126,  0.0172,  ...,  0.0073,  0.0216, -0.0023],\n",
       "        [ 0.0124,  0.0475, -0.0007,  ...,  0.0122,  0.0337,  0.0406],\n",
       "        ...,\n",
       "        [-0.0175,  0.0460,  0.0443,  ...,  0.0133,  0.0394, -0.0132],\n",
       "        [ 0.0716,  0.0349,  0.0206,  ...,  0.0549,  0.0485,  0.0493],\n",
       "        [ 0.0216, -0.0021,  0.0491,  ...,  0.0584,  0.0319,  0.0423]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.linear1.parametrizations.weight.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "\n",
    "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + \\\n",
    "                   net.linear1.parametrizations.weight[0].lora_B @ \\\n",
    "                    net.linear1.parametrizations.weight[0].lora_A * \\\n",
    "                    net.linear1.parametrizations.weight[0].scale \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "\n",
    "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 274.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9693\n",
      "Wrong count for 0: 14\n",
      "Wrong count for 1: 20\n",
      "Wrong count for 2: 25\n",
      "Wrong count for 3: 42\n",
      "Wrong count for 4: 27\n",
      "Wrong count for 5: 14\n",
      "Wrong count for 6: 21\n",
      "Wrong count for 7: 32\n",
      "Wrong count for 8: 75\n",
      "Wrong count for 9: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA enabled\n",
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:02<00:00, 362.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9663\n",
      "Wrong count for 0: 13\n",
      "Wrong count for 1: 35\n",
      "Wrong count for 2: 36\n",
      "Wrong count for 3: 30\n",
      "Wrong count for 4: 43\n",
      "Wrong count for 5: 20\n",
      "Wrong count for 6: 16\n",
      "Wrong count for 7: 29\n",
      "Wrong count for 8: 90\n",
      "Wrong count for 9: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA disabled\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
